\documentclass{article}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amsmath}
\usepackage{fancyhdr}

\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage[section]{placeins}

\sloppy
\definecolor{lightgray}{gray}{0.5}
\setlength{\parindent}{0pt}

\pagestyle{fancy}
\lhead{Page \thepage}

\renewcommand\thesubsection{\alph{subsection}.}
\renewcommand\thesubsubsection{\roman{subsubsection}.}




\title{HW4 \\
	\large CS 6210}
\date{11/11/2021}
\author{Spencer Peterson}

\pagestyle{fancy}
\rhead{Spencer Peterson}

\begin{document}
\maketitle


\section{Newton's Method}

The results of my experiment are shown below. The number of iterations is related to how far away the initial values are from the actual solution. 
\begin{center}
\begin{tabular}{ccccc}
Initial x1 & Initial x2 & num iterations & x1 Solution & x2 Solution\\
1 & 0.1 & 3 & 1 & 3.4005e-13\\
2 & -0.01 & 16 & 1 & 7.2916e-11\\
100 & 100 & 18 & 1 & -3.0633e-17\\
0 & 0 & 7 & 1 & 1.8265e-12\\
0 & 100 & 48 & 1 & 1.285e-14
\end{tabular}

\end{center}

\section{Harder Equations}


The result for the experiments for the normal set of hard equations are shown below. The result for when the initial values are -0.7 and 1.14 show a much higher number of iterations. Additionally, when running the experiment MATLAB raised a lot of warnings about inverting a singular matrix.
\begin{center}
\begin{tabular}{ccccc}
Initial x1 & Initial x2 & num iterations & x1 Solution & x2 Solution\\
1.29 & 0.58 & 16 & 1.2899 & 0.57987\\
1.1 & 1.1 & 24 & 1.2899 & 0.57987\\
2 & 0.5 & 22 & 1.2899 & 0.57987\\
3 & 5 & 29 & 1.2899 & 0.57987\\
-0.7 & 1.14 & 330 & 1.2899 & 0.57987
\end{tabular}

\end{center}

You cannot get a solution in the second case because the equations are not consistent. 

\section{LORAN}

Twenty five iterations is enough to get convergence with (400,400) as the initial value We get five other solutions with different starting values, additionally, not all of them converge within the 25 iteration limit that we set.

Some of the results with different initial positions are shown below.
\begin{center}
\begin{tabular}{ccccc}
Initial x1 & Initial x2 & num iterations & x1 Solution & x2 Solution\\
400 & 400 & 12 & 254.2211 & 219.307\\
420 & 420 & 25 & -1275.7285 & 1597.1663\\
430 & 430 & 25 & -193.2946 & 66.565\\
456 & 456 & 25 & 857.2773 & 1059.243\\
459 & 459 & 25 & 740.3652 & 906.8734
\end{tabular}

\end{center}


\section{Holmes Problem}

The gradient descent determined that x equals 0.8782 and y equals -0.003046 for the problem presented in Holmes 8.21. A table showing some of the steps of the gradient descent is shown below.
\begin{center}
\begin{tabular}{cccc}
n & epsilon & x & y \\
 1 & 1.440e+01 & 7.120e-01 & -8.000e-03\\ 
 2 & 5.120e-01 & 7.513e-01 & -1.480e-02\\ 
 3 & 3.790e-01 & 7.817e-01 & -1.853e-02\\ 
 4 & 2.885e-01 & 8.050e-01 & -2.119e-02\\ 
  \vdots &   \vdots &   \vdots &   \vdots \\
 77 & 3.246e-10 & 8.782e-01 & -3.046e-02\\ 
 78 & 2.445e-10 & 8.782e-01 & -3.046e-02\\ 
 79 & 1.842e-10 & 8.782e-01 & -3.046e-02\\ 
 80 & 1.388e-10 & 8.782e-01 & -3.046e-02\\ 
 81 & 1.045e-10 & 8.782e-01 & -3.046e-02\\ 
 82 & 7.874e-11 & 8.782e-01 & -3.046e-02
\end{tabular}

\end{center}

\section{Rosenbrock Problem}


\subsection{Gradient Descent}

Implementing the Gradient Descent algorithm for the Rosenbrock problem, we see that it takes approximately 43000 iterations to converge. The results with several starting positions are shown here.

\begin{center}
\begin{tabular}{cccccc}
$x_0$ & $y_0$ & n & eps & x & y\\
    0.500 &     0.500 & 42764 & 9.997e-09 &     1.000 &     1.000\\ 
    0.100 &     0.100 & 43505 & 9.998e-09 &     1.000 &     1.000\\ 
   -0.500 &     0.500 & 43920 & 9.999e-09 &     1.000 &     1.000
\end{tabular}

\end{center}

\subsection{Armijo Algorithm}

I found that my implementation of the Armijo Algorithm does not converge after 1,000,000 iterations for any of the starting points. It seems to oscillate around the solution without descending enough to find it.

\subsection{Adam Method}
This code was based off Adam: A Method for Stochastic Optimization. Specifically the algorithm shown on page 2. 

My Adam method had an similar problem. However the results it produced were closer, usually around 0.95 or 1.05 for both the x and y values. I think this may be because the momentum is carrying the calculated value past the solution.






\end{document}